---
title: "Appendix"
author: "-"
date: "June, 2019"
output: 
  bookdown::html_document2:
        toc: true
        number_sections: true
bibliography: library.bib
csl: apa.csl
link-citations: true
---

```{r,echo=FALSE,message=FALSE,warning=FALSE}

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,dpi=300,
                      fig.width = 10, fig.height = 6,cache=TRUE)

full_directory<-"./northquota/"
library(gridExtra)
library(abc)
library(abcrf)
library(glmnetUtils)
library(tidyverse)
library(stringr)


statistical_algorithms<- c("Logit","Historical","RUM - precise","RUM","RUM Fleetwide")
adaptive_algorithms<- c("EEI","EEI - Uncalibrated","Heatmap","Social Annealing")

levels=c("intercepts","clamped",
         "nofleetwide_identity","nofleetwide","fleetwide",
         "perfect","perfect_cell",
         "random","bandit","eei","default",
         "kernel","annealing")                             
labels_behaviour=c("Logit","Historical",
                   "RUM - precise","RUM","RUM Fleetwide",
                   
                   "Perfect - SA",
         "Perfect - Cell",
         "Random","Bandit",
         "EEI","EEI - Uncalibrated",
         "Heatmap","Social Annealing"
         )

#create helper to keep track of the name!
read_csv_filename <- function(filename){
  ret <- read_csv(filename)
  ret$Source <- filename #EDIT
  ret
}
```


# Algorithms Pseudocode



## Logit agents

Logit agents use the discrete choice model fit from the paper  to pick which statistical area to trawl next.  
More precisely, given a vector of logit coefficients $\beta$ and the design matrix $X$ where each row $x_i$ contains the observations regarding statistical area $i$, the probability of fishing in statistical area $i$ is:
$$
\text{Pr}(i) = \frac{e^{x_i \beta}}{ \sum_{j} e^{x_j \beta}}
$$
Once a statistical area is picked, a cell at random within that area is trawled

Expressed in pseudo-R, logit agents act as follows:
```{r logitagents, eval=FALSE, echo=TRUE}
# Given K statistical areas
# Logit coefficients beta
# And observations (distance, habit and intercepts) for each area 

#compute utility (logits)
for(statistical_area in 1:K)
  utility[statistical_area] = beta*observations[statistical_area,]

# Use SOFTMAX to pick one statistical area:
# probability of choosing arm i is proportional to the exp of its utility
denominator<- sum(exp(utility))
probabilities<- exp(utility)/denominator
# choose statistical area based on that probability
statistical_area<-sample(1:K,prob = probabilities,size = 1)
# target a random cell in that statistical area
target<-sample(cells[statistical_area],size=1)
```

This is the standard way to simulate historical behaviour in fishery agent-based models [see for example @saul_individual-based_2012; @Rose2015].
Because its $\beta$ vector comes from a statistical fit, it requires no further calibration.

## RUM Agents

RUM agents function just like logit agents but:

1. They use a larger set of independent variables $X$
2. Their parameters were tuned to minimize outcome error within POSEIDON

For these agents the $x$ vector in the logit probability is made up of the following:

1. Distance (in km) from homeport
2. Habit: number of times that location was fished in the past 365 days
3. Revenue/hr: 365 days moving average of revenues made at that location (ignoring all costs) divided by length of trips in hours
4. CPUE of
    * sablefish
    * Dover sole
    * yelloweye
    
There is a global intercept which is necessary for agents not to get stuck at the very first area they trawl.

There are two variants of the *RUM* agent: *RUM fleetwide* where the Revenue/hr $x$ averages across the experience of the whole fleet rather than individual observations and *RUM precise* where the agents perform the logit selection over the finer POSEIDON cells rather than the statistical areas.


## Historical agents  {#statisticallylockedagents}

Historical agents are a less elegant (but more effective) way to force agents to act historically.
Agents only consider statistical areas within 200km of their home port.
They target each area with probability proportional to how often that area was visited between 2011 to 2014.
For example, if the agent has to choose between statistical area *A* and *B*, and the logbook data shows that area *A* was visited 900 times while area *B* 100 times then the agent will visit *A* with probability $\frac{9}{10}$.  

Expressed more precisely in algorithm form:
```{r historicalagents, eval=FALSE, echo=TRUE}
# Given K statistical areas
# given trips[] vector containing empirically observed visits per statistical area

#ignore all areas that are too far
for(statistical_area in 1:K)
  if(distance(statistical_area) > 200)
     trips[statistical_area] = 0

# normalize trips into probabilities
trips<- trips/sum(trips)
# choose statistical area based on that probability
statistical_area<-sample(1:K,prob = trips,size = 1)
# target a random cell in that statistical area
target<-sample(cells[statistical_area],size=1)
```

If we judge statistical accuracy in terms of how much each statistical area was visited between 2011 and 2014, then the historical agent is more accurate than the logit one.
This is because logit agents have a high $\beta$ associated with "habit" variable and tend to often get stuck in suboptimal areas.
When we judge statistical accuracy by random-utility choice fits, then the logit agents are more accurate than the historical ones.  
Historical agents then look more accurate in aggregate but less so when studying their individual dynamics.

## Bandit agents

The multi-armed bandit problem  [@bubeck_regret_2012; @kuleshov_algorithms_2014; @vermorel_multi-armed_2005] is a classic  model of the exploration-exploitation tradeoff.
In its original form an agent faces a set of slot machines; each slot machine's payoff is stochastic but some machines are better than others. The agent can only play one slot machine at a time and has to balance the need to try new slot machines to learn their payoffs with the cost of not playing the slot machine that so far has proven superior.  
Many heuristics, bandit algorithms, have been proposed to solve this class of problem but here we focus only on the simplest one: the $\epsilon$ greedy bandit.

Each trip the agent has probability $\epsilon$ of trying a statistical area at random. If not the agent will go back fishing the area whose average profits have been highest so far.
At the end of each trip, the agent observes the profits made and updates its valuation of the statistical area visited.
More precisely, the algorithm proceeds as follows:
```{r epsilongreedy, eval=FALSE, echo=TRUE}
# Given K statistical areas
# given experience[] vector containing average profits observed so far by the agent

# with probability epsilon, make random choice
if(runif() < epsilon)
    chosen_area <- sample(1:K,size=1)
# otherwise pick the area that so far made the most profits
else
    chosen_area<-which.max(experience)  
    
target<-sample(cells[chosen_area],size=1)
    
# at the end of the trip, update your experience by 
# exponential moving average
experience[chosen_area]<- alpha * experience[chosen_area] + (1-alpha) * profits_made
```

The algorithm has 2 parameters: the exploration rate $\epsilon$ and the exponential moving average's smoothing parameter $\alpha$.



## Explore-Exploit-Imitate agents

Explore-exploit-imitate is the default algorithm in POSEIDON.
Much like the $\epsilon$-greedy algorithm, it has a fixed probability of exploring a new cell (although in this instance the exploration is in the neighborhood of the last cell visited, not a random new statistical area).  
When not exploring, the agent has a probability of imitating a "friend" who is making more money.
Otherwise the fisher targets the last cell trawled.

Each fisher has 2 "friends" she keeps track of.
Each fisher knows where their friends last trip was and how much money it made.
Friendship is only possible between boats of the same port.
Friendships are not necessarilly reciprocal.

In pseudo-R the explore-exploit-imitate algorithm looks as follows:
```{r, eval = FALSE, echo= TRUE}
# with probability epsilon, explore
if(runif()<epsilon)
  # pick a new area at random in the neighborhood of your last cell visited
  # where delta is the exploration range
  options<- von_neumann_neighborhood(last_cell_visited,
                                     delta)
  #go there
  target<-sample(options,size=1)
else
  # if a friend is doing better, target their area with probability gamma
  if(runif < gamma && profits[me] < profits[best_friend])
    target<- position[best_friend]
  # otherwise go back to the same cell
  else
    target<-last_cell_visited


#if explored and the profits are worse than before, then go back
if(profits(last_cell_visited)<profits(cell_visited_two_trips_ago))
  last_cell_visited<-cell_visited_two_trips_ago
```

The algorithm depends on 3 parameters: the exploration rate $\epsilon$, the exploration range $\delta$ and the imitation probability $\gamma$.

## Social annealing agents

Social annealing agents are a  slight variation over EEI where they do not know where other fishers trawl but they do know the profits others are making.
Whenever a social annealing agent is making $k$% worse than average fishery profits, she explores a new cell at random.
Otherwise she targets to the last cell visited.  

More precisely, the algorithm proceeds as follows:
```{r socialannealing, eval=FALSE, echo=TRUE}
# knowing last_cell_visited as the cell targeted in last trip

# if you are making less than k times the average
if(profit_made < k * average_profits)
  # pick a new area at random in the neighborhood of your last cell visited
  # where delta is the exploration range
  options<- von_neumann_neighborhood(last_cell_visited,
                                     delta)
  #go there
  target<-sample(options,size=1)
else
  #otherwise keep fishing in the same spot
  target<-last_cell_visited
```
The algorithm depends on two parameters: the threshold parameter $k$ and the exploration range (in map cells) $\delta$.
This algorithm first appeared (with $\delta=\infty$), to the best of our knowledge, in @Beecham2007.



## Heat-mapping agents

Heat-mapping agents build a statistical model of profit per cell (the eponymous heatmaps) and update it and the end of each trip.
With a fixed probability $1-\epsilon$ they target the peak of the heatmap otherwise fishing in its neighborhood.  

More precisely the heat-mapping agent builds a statistical heatmap by using a kernel regression where for each cell $x$ we initialize variables $g_0,m_0=0$.  
Whenever the fisher observes profits $\Pi_y$ at cell $y$, it updates its estimate profitability of cell $x$ by:
$$
\begin{aligned}
 g_n &= g_{n-1} + K(x,y) \\
 m_n &= m_{n-1} + \frac{ \left(\Pi_y - m_{n-1} \right) K(x,y)}{g_n}
\end{aligned}
$$
Where $K(\cdot,\cdot)$ is a kernel function and $m_n$ is the predicted profitability of $x$.  
To give more weight to more recent observations we add a forgetting factor $0<\lambda<1$ as follows:
$$
 g_n = \lambda g_{n-1} + K(x,x_n) 
$$
We use here as kernel function:
$$
K(x,y) = exp(-\frac{\text{Distance}(x,y)^2}{2\sigma})
$$
Where $\sigma$ is the space bandwitdh (that is, it modulates how much far away observations affect profitability predictions here).  
This recursive version of the Kernel regression first appeared in @krzyzak_global_1992.

Given the statistical model, the heat-mapping agents proceed as follows:
```{r, eval = FALSE, echo= TRUE}
# given heatmap_predictions: the predictions made by the fisher about each cell profitability
best_cell<-which.max(heatmap_predictions)
# with probability epsilon, explore around the best_cell
if(runif()<epsilon)
  # where delta is the exploration range
  options<- von_neumann_neighborhood(best_cell,
                                     delta)
  #go there
  target<-sample(options,size=1)
else
  # pick the top
  target<-best_cell
```

It is the strategy with more parameters to calibrate: $\delta$ is the exploration range, $\epsilon$ is the exploration probability, $\sigma$ is the space bandwitdh, $\lambda$ is the forgetting factor.

## Random agents

Random agent simply pick a cell at random each trip.
It is a very poor strategy but it is important to use as a baseline: if other algorithms' performance equals  the random agents' one then those algorithms aren't useful.
They have no parameters to calibrate


## Perfect SA agents 

Perfect-SA agents pick a random cell for each statistical area, compute the profitability for each of those cells and pick one in proportion to the exponential of the expected profits (like logit agents).
The agent has perfect knowledge in the sense that it knows ahead of time the profits she will make for each cell. 
However the agent does not update its computation halfway through a trip.
This means that if multiple perfect agents target the same cell, their profits might be lower than expected at the beginning of the trip.
These agents do not have the infinite computational power needed to analyse the game theoretical implications of everyone else being as informed as they do.  

More precisely, perfect agents act as follows:
```{r, eval = FALSE, echo= TRUE}
# given K statistical areas

#pick a cell for each area
for(statistical_area in 1:K)
  cell[statistical_area]<-sample(cells[statistical_area],size=1)
  #compute profits for each cell
  profits[statistical_area]<-forecast_profits(cell)

# Use SOFTMAX to pick one statistical area
denominator<- sum(exp(profits))
# probability of choosing arm i is proportional to the exp of its utility
probabilities<- exp(profits)/denominator
# choose statistical area based on that probability
statistical_area<-sample(1:K,prob = probabilities,size = 1)
# target selected cell in that statistical area
target<-cell[statistical_area]

```
 
## Perfect Cell Agents

Perfect cell agents just pick the POSEIDON cell that makes the most profit knowing ahead of time how much fish is there in each cell.
Like the *Perfect SA* agent it doesn't take into consideration other fishers and their actions. Unlike *Perfect SA* it does not use logit to make its decision stochastic.



# Original Validation

## Individual Errors


```{r errortable}
dashboards<-
  list.files(path=full_directory,
             pattern="*.csv",
             full.names = TRUE) %>%
  map_df(~read_csv_filename(.)) %>%
  separate(Source,c("path","name"),
           sep=full_directory) %>%
  dplyr::select(-path) %>%
  mutate(name=str_replace_all(name,".csv","") ) %>%
  mutate(name=str_replace_all(name,"/","") ) 

dashboards$name<-  factor(dashboards$name, levels=levels, 
                          labels=labels_behaviour)

summarise_with_error_final<-function(original,yearToFilter=c(6,7),
                                     north_quota=FALSE #sensitivity test, with north quota 
)
{
  sablefish_max_quota<- ifelse(north_quota,1606257,2724935)
  # average profits:  118,023 (126,432)
  simulation<-
    original %>%
    filter(year %in% yearToFilter) %>%
    group_by(run) %>%
    summarise_at(vars(-starts_with("name")),mean) %>%
    mutate(
      profit_error=abs(average_profits-134405.5)/21331 ,
      hours_out_error=abs(actual_hours_out-799.44)/120.382023907226 ,
      sole_error=abs(sole-6717.13*1000)/22234500/.0309569593683445 ,
      sablefish_error=abs(sablefish-1392.2001808742*1000)/sablefish_max_quota/0.06181  ,
      long_thornheads_error =abs(long_thornyheads-713.991403686*1000)/1966250/.0506622805119022,
      short_thornyheads_error = abs( short_thornyheads-734.20456815*1000 )/1481600.056/.050662280,
      rockfish_error = abs(rockfish - 0.0421841*1000)/600/.02 
      
    ) %>%
    
    mutate(
      error = 
        profit_error+
        hours_out_error +
        sole_error +
        sablefish_error +
        long_thornheads_error +
        short_thornyheads_error +
        rockfish_error
    )
  return(simulation)
}
errors<-(dashboards %>% group_by(name) %>% do(summarise_with_error_final(.,north_quota = TRUE)))


errors %>% group_by(name) %>% summarise_at(vars(ends_with("error")),median) %>% knitr::kable(
  col.names = c("Algorithm","Profit error","Hours out error","Sole error","Sablefish error",
                "Long Thornyheads error","Short thornyheads error","Rockfish error","Error"),digits=2
)


```

```{r individualerror}

## we can also look at error separately

disaggregated<-
  list.files(path=full_directory,
             pattern="*.csv",
             full.names = TRUE) %>%
  map_df(~read_csv_filename(.)) %>%
  separate(Source,c("path","name"),
           sep=full_directory) %>%
  dplyr::select(-path) %>%
  mutate(name=str_replace_all(name,".csv","") ) %>%
  mutate(name=str_replace_all(name,"/","") )  


disaggregated$name<-
  factor(disaggregated$name, levels=levels, 
         labels=labels_behaviour)



ggplot(disaggregated %>% filter(year>5) %>% filter(year<=7)) +
  geom_boxplot(aes(name,average_profits,fill=name))  +
  ylim(x=0,y=200000) +
  geom_hline(yintercept=134405.5,lwd=2,lty=2,col="red") +
  annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
  annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
  annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
  coord_flip() +
  scale_fill_discrete(guide=FALSE) +
  ylab("Average Profits per Boat per Year ($)") +
  xlab("Heuristic") + 
  theme_bw(20)

### a lot of variation is interannual; to prove this to yourself just try this instead:
# ggplot(disaggregated %>% filter(year>5) %>% filter(year<=7) %>% group_by(name,run) %>% summarise(average_profits=mean(average_profits))) +
#   geom_boxplot(aes(name,average_profits,fill=name))  +
#   ylim(x=0,y=200000) +
#   geom_hline(yintercept=134405.5,lwd=2,lty=2,col="red") +
#   annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
#   annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
#   annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
#   coord_flip() +
#   scale_fill_discrete(guide=FALSE) +
#   ylab("Average Profits per Boat per RUN ($)") +
#   xlab("Heuristic") + 
#   theme_bw(20)


## average hours out
ggplot(disaggregated %>% filter(year>5) %>% filter(year<=7)) +
  geom_boxplot(aes(name,actual_hours_out,fill=name))  +
  ylim(x=0,y=1200) +
  geom_hline(yintercept=799.44,lwd=2,lty=2,col="red") +
  annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
  annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
  annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
  coord_flip() +
  scale_fill_discrete(guide=FALSE) +
  ylab("Average Hours Out") +
  xlab("Heuristic") + 
  theme_bw(20)
## sole
ggplot(disaggregated %>% filter(year>5) %>% filter(year<=7)) +
  geom_boxplot(aes(name,sole/22234500,fill=name))  +
  ylim(x=0,y=1) +
  geom_hline(yintercept=6717.13*1000/22234500,lwd=2,lty=2,col="red") +
  annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
  annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
  annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
  coord_flip() +
  scale_fill_discrete(guide=FALSE) +
  ylab("Dover Sole Quota Attainment (%)") +
  xlab("Heuristic") + 
  theme_bw(20)

ggplot(disaggregated %>% filter(year>5) %>% filter(year<=7)) +
  geom_boxplot(aes(name,sablefish/1606257,fill=name))  +
  ylim(x=0,y=1) +
  geom_hline(yintercept=(1392.2001808742*1000)/1606257,lwd=2,lty=2,col="red") +
  annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
  annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
  annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
  coord_flip() +
  scale_fill_discrete(guide=FALSE) +
  ylab("Sablefish Quota Attainment (%)") +
  xlab("Heuristic")+ 
  theme_bw(20)

## long_thornheads_error =abs(long_thornyheads-713.991403686*1000)/1966250/.0506622805119022,
ggplot(disaggregated %>% filter(year>5) %>% filter(year<=7)) +
  geom_boxplot(aes(name,long_thornyheads/1966250,fill=name))  +
  ylim(x=0,y=1) +
  geom_hline(yintercept=(713.991403686*1000)/1966250,lwd=2,lty=2,col="red") +
  annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
  annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
  annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
  coord_flip() +
  scale_fill_discrete(guide=FALSE) +
  ylab("Long Thornyheads Quota Attainment (%)") +
  xlab("Heuristic")+ 
  theme_bw(20)


## short_thornyheads_error = abs( short_thornyheads-734.20456815*1000 )/1481600.056/.050662280,
ggplot(disaggregated %>% filter(year>5) %>% filter(year<=7)) +
  geom_boxplot(aes(name,short_thornyheads/1481600.056,fill=name))  +
  ylim(x=0,y=1) +
  geom_hline(yintercept=(734.20456815*1000)/1481600.056,lwd=2,lty=2,col="red") +
  annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
  annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
  annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
  coord_flip() +
  scale_fill_discrete(guide=FALSE) +
  ylab("Short Thornyheads Quota Attainment (%)") +
  xlab("Heuristic")+ 
  theme_bw(20)


ggplot(disaggregated %>% filter(year>5)) +
  geom_boxplot(aes(name,rockfish/600,fill=name))  +
  ylim(x=0,y=1) +
  geom_hline(yintercept=0.07,lwd=2,lty=2,col="red") +
  coord_flip() +
  scale_fill_discrete(guide=FALSE) +
  annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
  annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
  annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
  ylab("Yelloweye Quota Attainment (%)") +
  xlab("Heuristic")  + 
  theme_bw(20)



```


## Qualititative Validation


@Holland2016 tabulates the quota prices between 2011 and 2014 observed in the groundfish fishery.
We can compare this data with the quota prices generated from our model.
Holland however reports that a large amount of quotas are bartered rather than traded and that prices vary much during the year: only 18% of quota transfers in 2011 involved cash.

We look at the price of quotas traded within the simulation and compare them with Holland's numbers. 
Fundamentally, the constraining species in the model is sablefish whose quota is most expensive; the better the algorithm is at fishing, the more sablefish ends up costing. Other quotas are rarely traded and if so they are at the minimum price (0.05); this is true also for yelloweye which is so rare that is simply not traded as nobody really form high expectations of catching them.





```{r quotaprices, fig.cap="Five sets of box plots, one for each species' quota prices. The box plots show the yearly average trade price over 100 simulated runs. The red line is the 2011-2014 average as in table 2 of @Holland2016"}

### OTHER VALIDATIONS


dashboards<-
  list.files(path=full_directory,
             pattern="*.csv",
             full.names = TRUE) %>%
  map_df(~read_csv_filename(.)) %>%
  separate(Source,c("path","name"),
           sep=full_directory) %>%
  dplyr::select(-path) %>%
  mutate(name=str_replace_all(name,".csv","") ) %>%
  mutate(name=str_replace_all(name,"/","") ) %>%
  filter(name == "bandit" | name == "kernel" | name == "eei"  | name == "default"  )

dashboards$name<-factor(dashboards$name, levels=levels, 
                        labels=labels_behaviour)

simulation<-
  dashboards %>%
  filter(year>1) %>%
  filter(year<=5) %>%
  group_by(name,run) %>%
  summarise_all(median) %>%
  mutate(year = year + 2009) 



simulation_price<-
  simulation %>%
  dplyr::select(run,name,sable_price,yelloweye_price,short_price,long_price,doversole_price) %>%
  gather("Species","Quota Price",-name,-run) 

simulation_price$Species<-factor(simulation_price$Species,levels=c("sable_price",
                                                                   "yelloweye_price",
                                                                   "short_price",
                                                                   "long_price",
                                                                   "doversole_price"),
                                 labels = c("Sablefish","Yelloweye","Shortspine","Longspine","Dover Sole"))


simulation_price<-
  simulation_price %>%
  ungroup() %>%
  mutate(target = recode(Species,
                         `Sablefish`="2.1825764",
                         "Yelloweye"="66.69",
                         "Shortspine"="0.11",
                         "Longspine"="0.11",
                         "Dover Sole"="0.11" ))
simulation_price$target<-as.numeric(as.character(simulation_price$target))



ggplot(simulation_price) +
  geom_boxplot(aes(x=name,y=`Quota Price`,fill=name),col="black")  +
  expand_limits(x=0,y=0)+ expand_limits(x=4.3235295,y=0)+
  geom_hline(aes(yintercept=target),lwd=2,col="red",lty=2) +
  scale_fill_discrete(guide=FALSE) +
  facet_wrap(~Species,scales = "free_x",ncol=1) +
  ggtitle("Simulated Quota Prices")  +
  ylab("Quota Price($/kg)") +
  xlab("Decision-Making Algorithm") +
  coord_flip() 

```

As a second test we plot average daily profits over the years in figure \@ref(fig:avgprofits).
Data on profit trends from the FISHeries Economics Explorer dataset [@NorthwestFisheriesScienceCenter2017] shows a peak in variable costs net revenue in 2011, then a slight decline until another peak in 2015.
However the confidence intervals on the time series (with standard deviation approximately equal to the mean) are much larger than the trend itself which make any inference and calibration suspicious.
Our main focus here was to make sure there was movement of more than 2 or 3 times the mean itself.


```{r avgprofits, fig.cap= "The simulated profits time series, one for each adaptive decision-making algorithm"}


ggplot(dashboards %>% filter(year>1)) +
  geom_line(aes(x=year+2009,y=average_profits,group=run,col=name)) +
    scale_color_discrete(guide=FALSE) +
  ggtitle("Simulated Profits Time Series") +
  ylab("Average profits per year") +
  xlab("Simulation Years") +
  facet_wrap(~name)


locations<-
  read.csv("~/code/oxfish/inputs/california/dts_ports_2010.csv")  %>%
  rename(port=Port)

end_ports<-
  read.csv("~/code/oxfish/inputs/california/dts_ports_2014.csv")  %>%
  rename(port=Port)


original<-
  dashboards %>%
  filter(name=="EEI")

# original<-read_csv("/home/carrknight/code/oxfish/docs/20170730 validation/policies/eei_toscale.csv")


port_profits_early<-
  original %>% 
  filter(year==2) %>% 
  dplyr::select(contains("_fishers")) %>% gather("port","fishers") %>%
  separate(port,c("port","name"),sep="_") %>%
  dplyr::select(-name) %>%
  mutate(port=str_replace_all(port,"Coo.s.Bay","Coo's Bay")) %>%
  mutate(port=str_replace_all(port,"\\."," ")) %>%
  mutate(port=as.factor(port)) 


port_profits_early<-
  inner_join(port_profits_early,locations,by="port")  %>%
  arrange(desc(Northings)) %>%
  mutate(port = reorder(port,Northings))

labels<-levels(port_profits_early$port)

labels[labels=="San Francisco"] = "San Francisco*"
labels[labels=="Monterey"] = "Monterey*"

plot1<-
  ggplot(port_profits_early)+
  geom_boxplot(aes(port,fishers,fill=port)) +
  coord_flip() +
  ylim(0,16)+
  scale_fill_discrete(guide=FALSE) +
  ylab("# of Active Fishers") +
  scale_x_discrete(labels=labels) +
  xlab("Port") +
  annotate('text',x=1.5,y=10,label="* Catcher-Vessel Report quotes\n'<3' boats in SF and Monterey\nwe use 2 as a default value",col="black") 
#expand_limits(x=2,y=5)

port_profits_late<-
  original %>% 
  filter(year==5) %>% 
  dplyr::select(contains("_fishers")) %>% gather("port","fishers") %>%
  separate(port,c("port","name"),sep="_") %>%
  dplyr::select(-name) %>%
  filter(port!="average") %>%
  filter(port!="actual") %>%
  filter(port!="active") %>%
  mutate(port=str_replace_all(port,"Coo.s.Bay","Coo's Bay")) %>%
  mutate(port=str_replace_all(port,"\\."," ")) %>%
  mutate(port=as.factor(port)) 

port_profits_late<-
  left_join(port_profits_late,locations,by="port")  %>%
  arrange(desc(Northings)) %>%
  mutate(port = reorder(port,Northings))

plot2<-ggplot(port_profits_late)+
  geom_boxplot(aes(port,fishers,fill=port)) +
  coord_flip() +
  ylim(0,16)+
  scale_fill_discrete(guide=FALSE) +
  geom_point(data=end_ports,aes(x=port,y=Fishers),col="red",size=3.5,shape=15) +
  ylab("# of Active Fishers") +
  scale_x_discrete(labels=labels) +
  xlab("Port") +
  annotate('text',x=1.5,y=10,label="* Catcher-Vessel Report quotes\n'<3' boats in SF and Monterey\nwe use 2 as a default value",col="black") +
  xlab("Port")





```


As a third test we plot the number of fishers left in the simulation at the end of each year in figure \@ref(fig:activefishers).
From the catcher vessel report we know the number of fishers remaining in the DTS fishery [@Steiner2016], however these numbers change between each revision of the dataset.
Moreover each year about 10-15% of boats in the West-Coast groundfish fishery make operating losses so that our simulated quitting decision (making two consecutive years of losses) might be overly stringent.
Still, most simulations settle around the right number of fishers.


```{r activefishers, fig.cap= "Number of active fishers left in each simulation each year. The dashed red line shows the number of non-whiting DTS boats left by 2015."}
ggplot(dashboards %>% filter(year>1)) +
  geom_line(aes(x=year + 2009,y=as.integer(active_fishers),group=run,col=name)) +
  ggtitle("# of active fishers")+
  geom_hline(yintercept=47,lwd=2,lty=2,col="red") +
  facet_wrap(~name) +
  ylab("Active Fishers") +
  scale_color_discrete(guide=FALSE) +
  xlab("Year")
```

Finally, we plot in figure \@ref(fig:geographicaleei) the active fishers per port at the beginning and at the end of the simulation when using explore-exploit-imitate agents.
The geographical allocation of other adaptive algorithms is similar.
The model tends to predict too few fishers in Astoria and too many in Newport and Coo's Bay.
The model however does predict the decrease in DTS participation in Washington and is also accurate in  California.

```{r, geographicaleei, fig.cap= "Box-plot of active fishers per port (ordered from north to south along the coast) when the decision-making algorithm is the calibrated explore-exploit-imitate over 100 simulated runs. The red squares in the 2014 plot are the active fishers per port reported by @Steiner2016"}
grid.arrange(plot1 + 
               ggtitle("Initial Active Fishers"),
             plot2 +
               ggtitle("Active Fishers in year 2014"))
```




# Alternative Scenarios

## Pre-2011 Regulations


Because our model runs only for a short amount of time, we incur the risk of calibrating the model too tightly around the few years for which we have data. In this section we test for this risk by starting the model in 2007 rather than 2011. This involves changing starting abundance for each species, different gas prices, variable costs and implementing the 2011 regulatory shock when a different set of quotas were introduced and made tradeable. We run the model again until 2015 without recalibrating it. We test its prediction of both the 2011-2014 period (calibration error) and 2015 (validation error). 


Before 2011, the fishery was regulated through bi-monthly trip limits [@NationalMarineFisheriesService2009].
These rules changed frequently, were sometimes superseded by state regulations and varied between states.
We did not try to model these accurately as we used no pre-2011 data to calibrate or validate the model.
Our main focus was to have a different regime policy our agents would transition away from.

We follow again @Toft2011 here in simplifying these regulations as individual non-tradeable quotas.
We need to do so because we do not simulate explicitly fishing seasons and weather patterns.
Table \@ref(tab:iqquotas) shows the aggregate quota numbers we derived.
All quotas are higher except for Dover sole; however because Dover sole attainment rate is low, the overall effect of using pre-2011 quotas is more total landings.

Table: (\#tab:iqquotas) Yearly non-tradeable quotas to be shared among fishers each year pre-2011.

| Species    | Yearly Quota (mt) |
|------------|-------------------|
| Dover Sole | 16,866          |
| Sablefish  | 3,457.8           |
| Shortspine | 1,927.8            |
| Longspine  | 2,577.6          |
| Yelloweye  | 67.1               |




```{r sensitivityerror, fig.cap = "Box plot showing the fishing outcomes error for 100 simulations of each decision algorithm. All runs started in 2007 and transitioned to ITQ in 2011"}

sensitivity_directory<- paste(full_directory,"../northquota_pretopost/",sep="")

dashboards<-
  list.files(path=sensitivity_directory,
             pattern="*.csv",
             full.names = TRUE) %>%
  map_df(~read_csv_filename(.)) %>%
  separate(Source,c("path","name"),
           sep=sensitivity_directory) %>%
  dplyr::select(-path) %>%
  mutate(name=str_replace_all(name,".csv","") ) %>%
  mutate(name=str_replace_all(name,"_withscript","") ) %>%
  mutate(name=str_replace_all(name,"/","") ) 

dashboards$name<-factor(dashboards$name, levels=levels, 
                        labels=labels_behaviour)


sensitivity_validation_errors<-
  dashboards %>% group_by(name) %>%  do(summarise_with_error_final(.,yearToFilter = c(10,11),north_quota = TRUE))



ggplot(sensitivity_validation_errors) +
  geom_boxplot(aes(x=name,y=error,fill=name)) +
  ylim(0,80) +
  scale_fill_discrete(guide=FALSE) +
  coord_flip() +
  ylab("Error") +
  xlab("Heuristic") +
  ggtitle("Validation Errors - 2007 Starting Date")


```

All the results from validation are upheld. Adaptive agents do better, statistical agents underperform in 2015, perfect agents catch too much yelloweye and finally bandit and social annealers fit worse than in the calibration phase.  
The main difference is that *heatmap* agents now do better than *EEI*.
 


## CPUE map

In this section we change the distribution of fish while keeping the overall number and stock assessment-driven dynamics constant. We show that the model is sensitive to this change and that the main driver is the degree of smoothness by which fish is initially distributed.

Originally we distributed fish according to the Essential Fish Habitat statistical models; here we distribute fish proportional to the CPUE (catches per unit of effort) recorded in the logbook data for each statistical area.  
@branch_fleet_2006 provides plenty of reasons not to use CPUE as an abundance metric and we only do so for sensitivity analysis.  
Logbook observations are sparse so we apply a linear interpolator to have a CPUE prediction for each cell of the map.


The next figure shows how the abundance of sablefish changes as we change data source.
Because of the linear interpolation, the CPUE-derived map distributes fish much more smoothly.


```{r mapcomparison, fig.cap="Image capture of abundance for sablefish in the model as derived from the EFH dataset (on the left) and the CPUE interpolation (on the right)"}
knitr::include_graphics("/home/carrknight/code/oxfish/docs/paper3_dts/map-comparison.jpg")
```

Figure \@ref(fig:cpuemap) shows the errors obtained for each decision-making algorithm when calibrated against the new distribution of fish.  Three results emerge.
First, the error in absolute terms is much higher for all algorithms.
Second, statistical algorithms outperform adaptive algorithms.
Third, perfect agents perform as well as adaptive algorithms.
The only result that survives this sensitivity analysis is that random agents still perform very poorly.

```{r cpuemap, fig.cap= " The calibration errors when using the CPUE map, each box plot represents 100 simulation runs."}

directory_path <- paste(full_directory,"../northquota_map/",sep = "")


dashboard_errors<-
  list.files(path=directory_path,
             pattern="*.csv",
             full.names = TRUE) %>%
  map_df(~read_csv_filename(.)) %>%
  separate(Source,c("path","name"),
           sep=directory_path) %>%
  dplyr::select(-path) %>%
  mutate(name=str_replace_all(name,".csv","") ) %>%
  mutate(name=str_replace_all(name,"/","") ) %>%
  filter(name != "kernel2") %>%
  filter(name != "truly_perfect")

dashboard_errors$name<-
  factor(dashboard_errors$name, levels=levels, 
         labels=labels_behaviour)

errors <-(dashboard_errors %>% group_by(name) %>% do(summarise_with_error_final(.,north_quota = TRUE)))



cb_palette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442",
                "#0072B2", "#D55E00", "#CC79A7","#000000")

ggplot(errors) +
  geom_boxplot(aes(x=name,y=error,fill=name)) +
  #facet_grid(~type, scales = "free_y") +
  scale_fill_discrete(guide=FALSE) +
  coord_flip() +
  ylab("Error") +
  xlab("Algorithm") +
  ggtitle("Validation error with CPUE map")


```

All these results are due to the smoothness of geographical abundance.
Figure \@ref(fig:profitscpue) shows the profit simulated for each decision-making algorithm.
The figure makes clear that it is not the profit maximizer agent that got more realistic, it is adaptive agents that now also make too much money.
Smooth abundance distribution generates smooth profit curves that are easy for all adaptive agents to climb.
When the problem is easy, assuming adaptive agents is like assuming perfect agents.
Statistical agents perform well because they do not take advantange of the easier problem: they are forced to repeat historical choices even if better alternatives are present.


```{r profitscpue, fig.cap= "Yearly profits made by each decision-making algorithm in 100 simulated runs. Red dashed line is the empirical 2011-2014 average"}

uncalibrated_profits<-
  list.files(path=directory_path,
             pattern="*.csv",
             full.names = TRUE) %>%
  map_df(~read_csv_filename(.)) %>%
  separate(Source,c("path","name"),
           sep=directory_path) %>%
  dplyr::select(-path) %>%
  mutate(name=str_replace_all(name,".csv","") ) %>%
  mutate(name=str_replace_all(name,"/","") )  %>%
  filter(name != "kernel2") %>%
  filter(name != "truly_perfect")

uncalibrated_profits$name<-
  factor(uncalibrated_profits$name, levels=levels, 
         labels=labels_behaviour)

ggplot(uncalibrated_profits %>% filter(year>1) %>% filter(year<=5)) +
  geom_boxplot(aes(name,average_profits,fill=name))  +
  ylim(x=0,y=200000) +
  geom_hline(yintercept=89308,lwd=2,lty=2,col="red") +
  coord_flip() +
  scale_fill_discrete(guide=FALSE) +
  ylab("Average Profits per Boat per Year ($)") +
  xlab("Algorithm")

```


It is instructive to look in detail at the consequences of using CPUE-driven maps. 
One subtle property of the map is shortspine and sablefish being almost perfectly co-located.
This has less to do with the two species being actually co-located and instead simply represents the efficiency of fishermen in Astoria (a good reason for not using CPUE-driven maps).
The model reacts in a realistic way: because sablefish is the target species and it is now impossible landing it without catching shortspine as well, shortspine quotas become more expensive; as we show in figure \@ref(fig:shortspinequota) quotas are sometimes even traded at prices higher than shortspine's worth, meaning that agents take a loss by landing shortspine just so that they can land sablefish as well.



```{r shortspinequota, fig.cap = "Shortspine thornyhead quota prices traded for each simulation using either EFH or CPUE abundance maps. The red dashed line represents the empirical quota price observed, the dashed black line represents the ex-vessel price. With the CPUE-driven map we see not only quotas more expensive than what was empirically observed but sometimes more expensive than their sale prices"}


#cpue shortandsable
uncalibrated_profits<-
  list.files(path=directory_path,
             pattern="*.csv",
             full.names = TRUE) %>%
  map_df(~read_csv_filename(.)) %>%
  separate(Source,c("path","name"),
           sep=directory_path) %>%
  dplyr::select(-path) %>%
  mutate(name=str_replace_all(name,".csv","") ) %>%
  mutate(name=str_replace_all(name,"/","") )

# uncalibrated_profits$name<-
#   factor(uncalibrated_profits$name, levels=levels, 
#          labels=labels_behaviour)

uncalibrated_profits<-
  uncalibrated_profits %>%
  filter(name %in% c("eei","kernel","perfect","default","eei2")) %>%
  mutate(map="CPUE")

uncalibrated_profits$name<-
  factor(uncalibrated_profits$name, levels=levels, 
         labels=labels_behaviour)

calibrated_profits<-
  list.files(path=full_directory,
             pattern="*.csv",
             full.names = TRUE) %>%
  map_df(~read_csv_filename(.)) %>%
  separate(Source,c("path","name"),
           sep=full_directory) %>%
  dplyr::select(-path) %>%
  mutate(name=str_replace_all(name,".csv","") ) %>%
  mutate(name=str_replace_all(name,"/","") )  %>%
  filter(name != "kernel2") %>%
  filter(name != "truly_perfect")

calibrated<-
  calibrated_profits %>%
  filter(name %in% c("eei","kernel","perfect","default","eei2")) %>%
  dplyr::mutate(map="EFH")

calibrated$name<-
  factor(calibrated$name, levels=levels, 
         labels=labels_behaviour)

shortspine_prices<-
  bind_rows(calibrated,uncalibrated_profits) %>%
  group_by(run,map,name) %>%
  filter(year>1)  %>%
  mutate(short_price = ifelse(is.na(short_price),0,short_price)) %>%
  summarise(short_price=mean((short_price)))

ggplot(shortspine_prices ) +
  geom_boxplot(aes(name,short_price,fill=name))  +
  coord_flip() +
  scale_fill_discrete(guide=FALSE) +
  facet_grid( map~. ) +
  geom_hline(yintercept=0.05,lwd=2,col="red",lty=2) +
  # geom_hline(yintercept=1.0428510,lty=2) +
  ylab("Shortspine quota price ($/kg)") + 
  xlab("Algorithms")

```

It is not possible to speculate whether CPUE maps are always a poor idea for agent-based models.
In this fishery effort is clustered near the coast and around some key Oregon ports, while the EFH allocates fish further out and more homogeneously.
The issues with linear interpolations can also be prevented by higher resolution observations.

## Ignore boats making only one trip 

One problem with computing profits by averaging over boats is that there are a few agents who only make one trip each season filling only a limited amount of quota. The profits made by those boats are small and they tend to lower the average profit made by the fishery.  
We test this here by recalibrating the model to observe only profits of boats making more than one trip a season. 
This shuffles the ranking of adaptive agents as now *heatmap* agents do better than *EEI*, keeping most of the rest of the results constant.
The one major difference is that ranking by elastic nets in this instance prefers *historical* agents to adaptive ones.

```{r}
# focus on "actual profits" : profits made by boat who made more than 1 trip that year
# this required re-calibration because obviously profits computed like that
# are much higher as boats making only one trip aren't the most profitable ones 
# (otherwise they'd buy quota enough to do more than one trip!)

directory_path <- paste(full_directory,"../noonetrip/",sep = "")


dashboard_errors<-
  list.files(path=directory_path,
             pattern="*.csv",
             full.names = TRUE) %>%
  map_df(~read_csv_filename(.)) %>%
  separate(Source,c("path","name"),
           sep=directory_path) %>%
  dplyr::select(-path) %>%
  mutate(name=str_replace_all(name,".csv","") ) %>%
  mutate(name=str_replace_all(name,"/","") ) 

dashboard_errors$name<-
  factor(dashboard_errors$name, levels=levels, 
         labels=labels_behaviour)

summarise_with_error_final_noone<-function(original,yearToFilter=c(6,7),
                                           north_quota=FALSE #sensitivity test, with north quota 
)
{
  sablefish_max_quota<- ifelse(north_quota,1606257,2724935)
  # average profits:  118,023 (126,432)
  simulation<-
    original %>%
    filter(year %in% yearToFilter) %>%
    group_by(run) %>%
    summarise_at(vars(-starts_with("name")),mean) %>%
    mutate(
      profit_error=abs(actual_profits-134405.5)/21331 ,
      hours_out_error=abs(actual_hours_out-799.44)/120.382023907226 ,
      sole_error=abs(sole-6717.13*1000)/22234500/.0309569593683445 ,
      sablefish_error=abs(sablefish-1392.2001808742*1000)/sablefish_max_quota/0.06181  ,
      long_thornheads_error =abs(long_thornyheads-713.991403686*1000)/1966250/.0506622805119022,
      short_thornyheads_error = abs( short_thornyheads-734.20456815*1000 )/1481600.056/.050662280,
      rockfish_error = abs(rockfish - 0.0421841*1000)/600/.02 
      
    ) %>%
    
    mutate(
      error = 
        profit_error+
        hours_out_error +
        sole_error +
        sablefish_error +
        long_thornheads_error +
        short_thornyheads_error +
        rockfish_error
    )
  return(simulation)
}

errors <-(dashboard_errors %>% group_by(name) %>% do(summarise_with_error_final_noone(.,north_quota = TRUE)))



cb_palette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442",
                "#0072B2", "#D55E00", "#CC79A7","#000000")

ggplot(errors) +
  geom_boxplot(aes(x=name,y=error,fill=name)) +
  #facet_grid(~type, scales = "free_y") +
  scale_fill_discrete(guide=FALSE) +
  coord_flip() +
  ylab("Error") +
  xlab("Algorithm") +
  ggtitle("Validation error ignoring one trip boats")


#notice that in this case it's heatmap that does better!
#this should probably go to a table in the appendix
errors %>% group_by(name) %>% summarise_at(vars(ends_with("error")),median) %>% knitr::kable(
  col.names = c("Algorithm","Profit error","Hours out error","Sole error","Sablefish error",
                "Long Thornyheads error","Short thornyheads error","Rockfish error","Error"))

```

```{r}
## t-test
## you only want to test the "best" (Heatmap in this case) against all the others
top<-
  errors %>% dplyr::select(name,error) %>%
  filter(name=="Heatmap") %>% pull(error)


# wilcox
errors %>% dplyr::select(name,error) %>%
  group_by(name) %>%
  summarise(p_value=wilcox.test(x=error,y=top,conf.level = 1-0.05/12)$p.value) %>% #bonferroni's correction 
mutate(p_value = format.pval(p_value)) %>%
  knitr::kable(caption="Wilcox Test p-values")

```
```{r results="hide"}
## ABC
## standard rejection

abc_ready<-errors  %>%
  dplyr::select(actual_profits,actual_hours_out,sole,sablefish,long_thornyheads,short_thornyheads,rockfish,name)

target<-c(
  134405.5,799.44,6717.13*1000,1392*1000,713.99*1000,734.20*1000,0.0421841*1000
)
done<-postpr(target,index = abc_ready %>% pull(name) %>% as.vector(), 
             abc_ready %>% ungroup() %>% dplyr::select(-name),
             method="rejection",
             tol=.1)
done<-summary(done)



## abc RF


rffit<-abcrf::abcrf(name~.,data=abc_ready %>% ungroup())
names(target)<-
  c("actual_profits","actual_hours_out","sole","sablefish","long_thornyheads","short_thornyheads","rockfish")
target %>% t() %>% as.data.frame()
predict(rffit,obs=target %>% t() %>% as.data.frame(),training= abc_ready %>% ungroup()) -> done2
votes<-done2$vote/sum(done2$vote,na.rm = TRUE)

## elastic net
ernesto<-
  glmnetUtils::cv.glmnet(
    formula=as.formula(paste("name~(",paste(names(target),collapse="+"),")^2")),
    data=abc_ready %>% ungroup(),
    family="multinomial"
  )

predict(ernesto,newdata=target %>% t() %>% as.data.frame(),type="response",s="lambda.1se") ->done3
#coef(ernesto,s="lambda.1se")

```

```{r}
#but elastic nets give you historical being most likely here! 
rbind(
  
  done$Prob %>% t() %>% as.data.frame(),
  (done2$vote/sum(done2$vote)) %>% as_data_frame(),
  done3[,,1] %>% t() %>% as_data_frame()
) %>% mutate(method=c("ABC","RF","Elastic Nets")) %>% knitr::kable(digits=3)



```

## Unified sablefish market

In the model we remove quotas that eventually were landed by other gears. This is accurate but had we truly looked into the future rather than hindcasting we would have not had that information. Would behavioural ranking change if we made north and south sablefish quota all available to the DTS boats?
Behavioural ranking does not change but errors are in general all higher due to too much sablefish being landed.


```{r}
full_directory<-"./longrun/"


#full_directory <- "/home/carrknight/code/oxfish/docs/20170730 validation/best-inference/20170822_dryrun/"
logbook_directory <- paste(full_directory,
                           "../logbook/",sep="")

#create helper to keep track of the name!
read_csv_filename <- function(filename){
  ret <- read_csv(filename)
  ret$Source <- filename #EDIT
  ret
}

summarise_with_error<-function(original,initialYear=1,finalYear=5,
                               north_quota=FALSE #sensitivity test, with north quota 
)
{
  sablefish_max_quota<- ifelse(north_quota,1606257,2724935)
  simulation<-
    original %>%
    filter(year>initialYear) %>%
    filter(year<=finalYear) %>%
    group_by(run) %>%
    summarise_at(vars(-starts_with("name")#,-"run"
    ),mean) %>%
    mutate(
      error = 
        abs(average_profits-118552)/21331 +
        abs(actual_hours_out-999.936)/120.382023907226 +
        abs(sole/22234500-0.3325)/.0309569593683445 +
        abs(sablefish/sablefish_max_quota-.836557879240702)/0.06181 +
        abs(short_thornyheads/1481600.056-0.525)/.0506622805119022 +
        abs(long_thornyheads/1966250-0.515)/.0506622805119022 +
        abs(rockfish/600-0.07)/.02 +
        abs(avg_duration-69.097625)/33 +
        abs(avg_distance-90.88762)/32
    )
  return(simulation)
}

# get all runs!


logbook_errors<-
  list.files(path=logbook_directory,
             pattern="*.csv",
             full.names = TRUE) %>%
  map_df(~read_csv_filename(.)) %>%
  separate(Source,c("path","name"),
           sep=full_directory) %>%
  dplyr::select(-path) %>%
  mutate(name=basename(name) ) %>%
  mutate(name=str_replace_all(name,".csv","") ) %>%
  mutate(name=str_replace_all(name,".yaml","") ) %>%
  mutate(name=str_replace_all(name,"/","") )


logbook_errors<-
  logbook_errors %>%
  mutate(error = (abs((habit-0.212637158885711) / 0.0130763512467314)  
                  + abs((distance + 0.0210724717780951) / 0.00139610268440124) ))

logbook_errors$name <-
  factor(logbook_errors$name, levels=levels, 
         labels=labels_behaviour)


### OUTCOME ERRORS

dashboard_errors<-
  list.files(path=full_directory,
             pattern="*.csv",
             full.names = TRUE) %>%
  map_df(~read_csv_filename(.)) %>%
  separate(Source,c("path","name"),
           sep=full_directory) %>%
  dplyr::select(-path) %>%
  mutate(name=str_replace_all(name,".csv","") ) %>%
  mutate(name=str_replace_all(name,"/","") ) 

dashboard_errors$name<-
  factor(dashboard_errors$name, levels=levels, 
         labels=labels_behaviour)

dashboard_errors<-(dashboard_errors %>% group_by(name) %>% do(summarise_with_error(.)))

dashboard_errors<-dashboard_errors


errors<-
  bind_rows("Logbook Error"=logbook_errors,
            "Outcome Error"=dashboard_errors,
            .id="type")  %>%
  mutate(decision_type=case_when(
    name %in% statistical_algorithms ~ "Statistical",
    name %in% adaptive_algorithms ~ "Adaptive",
    TRUE ~ "Other"
  ))

errors$type<-factor(errors$type,levels = c("Logbook Error","Outcome Error"))

cb_palette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442",
                "#0072B2", "#D55E00", "#CC79A7","#000000")




ggplot(errors) +
  geom_boxplot(aes(x=name,y=error,fill=name)) +
  ylim(0,80) +
  facet_grid(~type, scales = "free_x") +
  scale_fill_discrete(guide=FALSE) +
  annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
  annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
  annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
  #geom_vline(xintercept=c(2.5,4.5),lty=2) +
  coord_flip() +
  ylab("Error") +
  xlab("Heuristic") +
  ggtitle("Calibration (in-sample) Errors") +
  theme_gray(20)

### now we can look at validation error SEPARATELY

uncalibrated_profits<-
  list.files(path=full_directory,
             pattern="*.csv",
             full.names = TRUE) %>%
  map_df(~read_csv_filename(.)) %>%
  separate(Source,c("path","name"),
           sep=full_directory) %>%
  dplyr::select(-path) %>%
  mutate(name=str_replace_all(name,".csv","") ) %>%
  mutate(name=str_replace_all(name,"/","") )  %>%
  filter(name != "kernel2") %>%
  filter(name != "truly_perfect")

uncalibrated_profits$name<-
  factor(uncalibrated_profits$name, levels=levels, 
         labels=labels_behaviour)

## profits

ggplot(uncalibrated_profits %>% filter(year>5) %>% filter(year<=7)) +
  geom_boxplot(aes(name,average_profits,fill=name))  +
  ylim(x=0,y=200000) +
  geom_hline(yintercept=134405.5,lwd=2,lty=2,col="red") +
  annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
  annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
  annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
  coord_flip() +
  scale_fill_discrete(guide=FALSE) +
  ylab("Average Profits per Boat per Year ($)") +
  xlab("Heuristic") + 
  theme_bw(20)
## average hours out
ggplot(uncalibrated_profits %>% filter(year>5) %>% filter(year<=7)) +
  geom_boxplot(aes(name,actual_hours_out,fill=name))  +
  ylim(x=0,y=1200) +
  geom_hline(yintercept=799.44,lwd=2,lty=2,col="red") +
  annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
  annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
  annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
  coord_flip() +
  scale_fill_discrete(guide=FALSE) +
  ylab("Average Hours Out") +
  xlab("Heuristic") + 
  theme_bw(20)
## sole
ggplot(uncalibrated_profits %>% filter(year>5) %>% filter(year<=7)) +
  geom_boxplot(aes(name,sole/22234500,fill=name))  +
  ylim(x=0,y=1) +
  geom_hline(yintercept=6717.13*1000/22234500,lwd=2,lty=2,col="red") +
  annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
  annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
  annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
  coord_flip() +
  scale_fill_discrete(guide=FALSE) +
  ylab("Dover Sole Quota Attainment (%)") +
  xlab("Heuristic") + 
  theme_bw(20)

### HERE YOU CAN SEE THE PROBLEM OF NOT REMOVING EXOGENOUSLY THE QUOTA SUCKED UP BY LONGLINERS
ggplot(uncalibrated_profits %>% filter(year>5) %>% filter(year<=7)) +
  geom_boxplot(aes(name,sablefish/2724935,fill=name))  +
  ylim(x=0,y=1) +
  geom_hline(yintercept=(1392.2001808742*1000)/2724935,lwd=2,lty=2,col="red") +
  annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
  annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
  annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
  coord_flip() +
  scale_fill_discrete(guide=FALSE) +
  ylab("Sablefish Quota Attainment (%)") +
  xlab("Heuristic")

## long_thornheads_error =abs(long_thornyheads-713.991403686*1000)/1966250/.0506622805119022,
ggplot(uncalibrated_profits %>% filter(year>5) %>% filter(year<=7)) +
  geom_boxplot(aes(name,long_thornyheads/1966250,fill=name))  +
  ylim(x=0,y=1) +
  geom_hline(yintercept=(713.991403686*1000)/1966250,lwd=2,lty=2,col="red") +
  annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
  annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
  annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
  coord_flip() +
  scale_fill_discrete(guide=FALSE) +
  ylab("Long Thornyheads Quota Attainment (%)") +
  xlab("Heuristic")


## short_thornyheads_error = abs( short_thornyheads-734.20456815*1000 )/1481600.056/.050662280,
ggplot(uncalibrated_profits %>% filter(year>5) %>% filter(year<=7)) +
  geom_boxplot(aes(name,short_thornyheads/1481600.056,fill=name))  +
  ylim(x=0,y=1) +
  geom_hline(yintercept=(734.20456815*1000)/1481600.056,lwd=2,lty=2,col="red") +
  annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
  annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
  annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
  coord_flip() +
  scale_fill_discrete(guide=FALSE) +
  ylab("Short Thornyheads Quota Attainment (%)") +
  xlab("Heuristic")


ggplot(uncalibrated_profits %>% filter(year>5)) +
  geom_boxplot(aes(name,rockfish/600,fill=name))  +
  ylim(x=0,y=1) +
  geom_hline(yintercept=0.07,lwd=2,lty=2,col="red") +
  coord_flip() +
  scale_fill_discrete(guide=FALSE) +
  annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
  annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
  annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
  ylab("Yelloweye Quota Attainment (%)") +
  xlab("Heuristic")  + 
  theme_bw(20)

## VALIDATION



dashboards<-
  list.files(path=full_directory,
             pattern="*.csv",
             full.names = TRUE) %>%
  map_df(~read_csv_filename(.)) %>%
  separate(Source,c("path","name"),
           sep=full_directory) %>%
  dplyr::select(-path) %>%
  mutate(name=str_replace_all(name,".csv","") ) %>%
  mutate(name=str_replace_all(name,"/","") ) 

dashboards$name<-  factor(dashboards$name, levels=levels, 
                          labels=labels_behaviour)

summarise_with_error_final<-function(original,yearToFilter=c(6,7),
                                     north_quota=FALSE #sensitivity test, with north quota 
)
{
  sablefish_max_quota<- ifelse(north_quota,1606257,2724935)
  # average profits:  118,023 (126,432)
  simulation<-
    original %>%
    filter(year %in% yearToFilter) %>%
    group_by(run) %>%
    summarise_at(vars(-starts_with("name")),mean) %>%
    mutate(
      profit_error=abs(average_profits-134405.5)/21331 ,
      hours_out_error=abs(actual_hours_out-799.44)/120.382023907226 ,
      sole_error=abs(sole-6717.13*1000)/22234500/.0309569593683445 ,
      sablefish_error=abs(sablefish-1392.2001808742*1000)/sablefish_max_quota/0.06181  ,
      long_thornheads_error =abs(long_thornyheads-713.991403686*1000)/1966250/.0506622805119022,
      short_thornyheads_error = abs( short_thornyheads-734.20456815*1000 )/1481600.056/.050662280,
      rockfish_error = abs(rockfish - 0.0421841*1000)/600/.02 
      
    ) %>%
    
    mutate(
      error = 
        profit_error+
        hours_out_error +
        sole_error +
        sablefish_error +
        long_thornheads_error +
        short_thornyheads_error +
        rockfish_error
    )
  return(simulation)
}
errors<-(dashboards %>% group_by(name) %>% do(summarise_with_error_final(.)))




ggplot(errors) +
  geom_boxplot(aes(x=name,y=error,fill=name)) + coord_flip() +
  scale_fill_discrete(guide=FALSE) + 
  ylab("Validation Error") +
  xlab("Heuristic") +
  ggtitle("2015-2016 Validation Error") +
  annotate("rect",xmin=0,xmax=5.5,ymin=-Inf,ymax=Inf,fill="red",lty=2,alpha=.1) +
  annotate("rect",xmin=5.5,xmax=8.5,ymin=-Inf,ymax=Inf,fill="green",lty=2,alpha=.1) +
  annotate("rect",xmin=8.5,xmax=Inf,ymin=-Inf,ymax=Inf,fill="blue",lty=2,alpha=.1) +
  theme_bw(20)



```

```{r}

#this should probably go to a table in the appendix
errors %>% group_by(name) %>% summarise_at(vars(ends_with("error")),median) %>% knitr::kable(
  col.names = c("Algorithm","Profit error","Hours out error","Sole error","Sablefish error",
                "Long Thornyheads error","Short thornyheads error","Rockfish error","Error")
)

```


```{r results="hide"}
## ABC
## standard rejection

abc_ready<-errors  %>%
  dplyr::select(average_profits,actual_hours_out,sole,sablefish,long_thornyheads,short_thornyheads,rockfish,name)

target<-c(
  134405.5,799.44,6717.13*1000,1392*1000,713.99*1000,734.20*1000,0.0421841*1000
)
done<-postpr(target,index = abc_ready %>% pull(name) %>% as.vector(), 
             abc_ready %>% ungroup() %>% dplyr::select(-name),
             method="rejection",
             tol=.1)
done<-summary(done)



## abc RF


rffit<-abcrf::abcrf(name~.,data=abc_ready %>% ungroup())
names(target)<-
  c("average_profits","actual_hours_out","sole","sablefish","long_thornyheads","short_thornyheads","rockfish")
target %>% t() %>% as.data.frame()
predict(rffit,obs=target %>% t() %>% as.data.frame(),training= abc_ready %>% ungroup()) -> done2
votes<-done2$vote/sum(done2$vote,na.rm = TRUE)

## elastic net
ernesto<-
  glmnetUtils::cv.glmnet(
    formula=as.formula(paste("name~(",paste(names(target),collapse="+"),")^2")),
    data=abc_ready %>% ungroup(),
    family="multinomial"
  )

predict(ernesto,newdata=target %>% t() %>% as.data.frame(),type="response",s="lambda.1se") ->done3
#coef(ernesto,s="lambda.1se")

```

```{r}
rbind(
  
  done$Prob %>% t() %>% as.data.frame(),
  (done2$vote/sum(done2$vote)) %>% as_data_frame(),
  done3[,,1] %>% t() %>% as_data_frame()
) %>% mutate(method=c("ABC","RF","Elastic Nets")) %>% knitr::kable(digits=3)



```



# Active non-linear tests


@miller_active_1998 introduces active non-linear tests as a search problem [see chapters 3-5 of @Russell2009]: look in the neighborhood of the optimal parameters for the worst possible outcome.
In its original implementation a search around 10% neighborhood of calibrated parameters for the WORLD3 model [@Meadows1972] delivered predictions completely at odds with the authors' original claims.
@Stonedahl2010 provides another example searching in the 10% neighborhood of the Anasazi model [see also @Lee2015].  
Here we use active non-linear tests to study the sensitivity of our calibrated parameters.


## Historical

```{r}
original <- read_csv(paste(full_directory,"clamped.csv",sep=""))
ant <- read_csv(paste(full_directory,"../ants/ant_historical/ant_worse.csv",sep=""))
original<-summarise_with_error(original)
ant<-summarise_with_error(ant)
original$name<-"Calibrated"
ant$name<-"Active Nonlinear Test"

test<-bind_rows(original,ant)

```

We first perform an active non-linear test to the calibrated catchability ($Q$) and hold-size ($H$) parameters by varying them 10% around their optimal value with *historical* agents.
We show the density plot of fishing outcome error in figure \@ref(fig:antstep).
The search increases the error by only 11% on average. While the difference is significant to a Wilcox test, it is linearly proportional to the 10% shock in parameters. 



```{r antstep, fig.cap = " The histogram of fishing outcome errors generated by 100 runs of the step 1 calibrated model (that is, using historical agents) at the bottom versus the worst performing set of parameters found by the active non-linear test." }
ggplot(test) +
  geom_density(aes(error,fill=name),col="black") +
  facet_grid(name~.) +
  xlab("Outcome Error") +
  scale_fill_discrete(guide=FALSE) +
  ylab("Frequency") +
  ggtitle("Step 1 Sensitivity")

#wilcox.test(original$error,ant$error)

```


## EEI

Explore-exploit-imitate agents are very insensitive to parameter changes. 
In figure \@ref(fig:eeiantt) we compare the calibrated and worst possible parameter sets (changing behavioural parameters by at most 10%). The error is statistically significant to a Wilcoxon test, but the overall increase is only of 6%.
Even though the explore-exploit-imitate sensitivity analysis has more parameters it is actually stabler the historical agents used above.

```{r eeiantt, fig.cap = "The density plot of fishing outcome errors generated by 100 runs of the step 2 calibrated model using explore-exploit-imitate at the bottom versus the worst performing set of parameters found by the active non-linear test."  }
original <- read_csv(paste(full_directory,"eei.csv",sep=""))
ant <- read_csv(paste(full_directory,"../ants/ant_eei/ant_worse.csv",sep=""))
original<-summarise_with_error(original)
ant<-summarise_with_error(ant)
original$name<-"Calibrated"
ant$name<-"Active Nonlinear Test"

test<-bind_rows(original,ant)
ggplot(test) +
  geom_density(aes(error,fill=name),col="black") +
  facet_grid(name~.) +
  xlab("Outcome Error") +
  scale_fill_discrete(guide=FALSE) +
  ylab("Density") +
  ggtitle("Explore-Exploit-Imitate, Total Sensitivity")

#wilcox.test(original$error,ant$error)
#abs(mean(original$error)-mean(ant$error))/mean(original$error)

```


## Kernel

Heat-mapping agents are more sensitive to active non-linear tests (particularly to changes in forgetting factor $\lambda$).
As shown in figure \@ref(fig:kernelantt) the average fishing outcome error increases only by 13% but the likelihood of large errors is higher.
In a way this is stating the obvious: heat-mapping agents are more complex than explore-exploit-imitate ones and we thus expect them to be more delicate to parameter changes.


```{r kernelantt,  fig.cap="The density plot of fishing outcome errors generated by 100 runs of the step 2 calibrated model using heatmap agents at the bottom versus the worst performing set of parameters found by the active non-linear test."}

original <- read_csv(paste(full_directory,"kernel.csv",sep=""))
ant <- read_csv(paste(full_directory,"../ants/ant_heatmap/kernel_worst.csv",sep=""))
original<-summarise_with_error(original)
ant<-summarise_with_error(ant)
original$name<-"Calibrated"
ant$name<-"Active Nonlinear Test"

test<-bind_rows(original,ant)
ggplot(test) +
  geom_density(aes(error,fill=name),col="black") +
  facet_grid(name~.) +
  xlab("Outcome Error") +
  scale_fill_discrete(guide=FALSE) +
  ylab("Frequency") +
  ggtitle("Heat-mapping Agent, Total Sensitivity")


```


## Movement Test


From Alaskan tagging data we know that sablefish moves on average 191 km per year [@Hanselman2015a].
We also know from simple abstract model runs that fish diffusion rates in POSEIDON have large effects on long term profitability and behaviour of the simulated fleet.
For these reasons we initially classed sablefish movement as a free parameter to calibrate much like catchabilities and hold-sizes.
In these early calibration runs however movement rate did not have a significant effect in minimizing distance from data (which in a Bayesian optimization can be noticed by the very large bandwitdhs assigned to the variable by the optimiser or just by plotting posteriors).
In the final version of the calibration presented above the movement rate was simply set to 0 as it made the model much faster to run.


Similar to Goethel et al. (2011), we modeled movement as a simple diffusion rate between cells.
However, we biased movement in such a way that it would never alter the patchiness of the
original map. 
Movement in this simulation is not from cells where there are more fish to cells
where there are a few; rather fish moves from full areas to empty ones where full and empty are defined in terms initial abundance distribution.
To make a concrete example, take cell A and a contiguous cell B where cell A contained $k_a$
percent of the total sablefish at the beginning of the simulation and cell B contained $k_b$ percent.
If there currently are $f_a$ sablefish  in cell A and $f_b$ in cell B, then fish movement between A and B is given by:
\[ 
m \frac{f_a k_b - f_b k_a}{k_a+k_b}
\]
where $m \in [0,1]$ is the diffusion rate parameter. This means that even at maximum movement
rate distribution of fish remains as patchy.

Figure \@ref(fig:movementcalibration) shows the difference in error generated by maximizing sablefish diffusion rate.
Validation error is fundamentally unchanged. Calibration error (not shown) increases by 9%.
While we know that POSEIDON is sensitive to movement rates in other scenarios here we showed that when simulating short periods of time there is little error added by ignoring movement and it is anyway hard to back out a proper movement rate from data without longer observations.

```{r movementcalibration, fig.cap = "Density plot of validation error generated by 100 runs with movement turned off and with movement turned on, $m=0.1$"}

original <- read_csv(paste(full_directory,"clamped.csv",sep=""))
ant <- read_csv(paste(full_directory,"../ants/movement_test/clamped_plus_movement.csv",sep=""))
original<-summarise_with_error_final(original)
ant<-summarise_with_error_final(ant)
original$name<-"No Movement"
ant$name<-"Movement"

test<-bind_rows(original,ant)


test<-bind_rows(original,ant)
ggplot(test) +
  geom_density(aes(error,fill=name),col="black") +
  facet_grid(name~.) +
  xlab("Validation Error") +
  scale_fill_discrete(guide=FALSE) +
  ylab("Frequency") +
  ggtitle("Validation error sensitivty to movement")


```

# Alternative Behavioural Parameters

## Validation Minimizing RUM

RUM agents perform poorly in the validation section primarily by overfitting.  Would they have been much different if we had "cheated" and calibrated them against validation error?
Statistical area agents (RUM and RUM SA) see almost no improvement but *RUM precise* agents can halve their validation error if reparametrized. Figure \@ref(fig:rumantt) shows the difference in calibration and validation error distribution for the original *RUM precise* agent (whose parameters minimize calibration error) and the "cheating" *RUM precise* agent (whose parameters minimize validation error).



```{r rumantt, fig.cap= "Empirical densities of validation and calibration error for *RUM precise* agents. As expected the cheating agent sacrifices fit quality in the calibration phase in order to do better in validation."}

full_directory<-"./northquota/"



original<- read_csv(paste0(full_directory,"",sep="nofleetwide_identity.csv")) %>%
  summarise_with_error_final(north_quota = TRUE) %>% mutate(id="Original",type="Validation") %>%
  dplyr::select(id,error,type)
cheating <- read_csv(paste(full_directory,"../ants/cheating_northquota/nofleetwide_identity_cheating.csv",sep="")) %>%
  summarise_with_error_final(north_quota = TRUE) %>%mutate(id="Cheating",type="Validation") %>%
  dplyr::select(id,error,type) 
  
original_c<- read_csv(paste0(full_directory,"",sep="nofleetwide_identity.csv")) %>%
  summarise_with_error(north_quota = TRUE) %>% mutate(id="Original",type="Calibration") %>%
  dplyr::select(id,error,type)
cheating_c <- read_csv(paste(full_directory,"../ants/cheating_northquota/nofleetwide_identity_cheating.csv",sep="")) %>%
  summarise_with_error(north_quota = TRUE) %>%mutate(id="Cheating",type="Calibration") %>%
  dplyr::select(id,error,type) 


bind_rows(original,cheating,original_c,cheating_c) %>%
  ggplot(aes(x=error,fill=id),col="black") +
  geom_density(alpha=0.8) +
  facet_grid(type~.) +
  theme_bw(20) +
  xlab("Outcome error") +
  ylab("Density") 

```

The key result of this exercise is to compare the optimal parameters for the original RUM precise and the one that minimizes validation error instead:

| Parameter       | Original | Validation Minimizer |
|-----------------|----------|----------------------|
| Dover sole CPUE | -0.5112  | -0.0773              |
| Sablefish CPUE  | -0.0086  | 0.55                 |
| Yelloweye CPUE  | -0.345   | 0.714                |
| Distance        | -0.0074  | -0.01                |
| *Habit*         |**2.4855**| **-2.5425**        |
| Revenue         | 0.6633   | 0.4148               |
| Intercept       | 45.58    | 44.07                |

The main change is the difference in habit which goes from very positive to very negative (compare this to the 0.21 coefficient in the logbook-derived *Logit* agent). This is introducing purposeful exploration "by the backdoor": it is a way to keep the agent changing its destination even if the other indicators do not change. 

This exploration makes a difference in even the few years that are simulated as figure \@ref(fig:rumprofitss) shows: profits decrease much faster for the original *RUM Precise* than the cheating one which are pushed to move around over time.

```{r rumprofitss, fig.cap= "Empirical densities of validation and calibration error for *RUM precise* agents. As expected the cheating agent sacrifices fit quality in the calibration phase in order to do better in validation."}
original<- read_csv(paste0(full_directory,"../northquota/nofleetwide_identity.csv")) %>% 
  mutate(id="Original",type="Validation") %>%
  dplyr::select(id,average_profits,type,run,year)
cheating <- read_csv(paste(full_directory,"../ants/cheating_northquota/nofleetwide_identity_cheating.csv",sep="")) %>%
  mutate(id="Cheating",type="Validation") %>%
  dplyr::select(id,average_profits,type,run,year)


ggplot(bind_rows(original,cheating) %>% filter(year>1) %>% mutate(year=year+2009))   +
  geom_line(aes(x=year,y=average_profits,col=id,group=interaction(run,id))) +
  ylab("Average Profits ($)") +
  xlab("Simulation Year") +
  scale_color_discrete(name="Agent") +
  ggtitle("Simulated profits for RUM precise agents")

```

## Validation Minimizing Social Annealing

Social annealers perform poorly because the parameters for which they are calibrated ($k=0.548$ and $\delta=1$) generate very easily satisfied agents who have little reason to explore and improve fishing efficiency. This results in a steady decline of catches, profits and eventually number of active fishers.

If we "cheat" and look instead for the parameters that minimize validation error we see in figure \@ref(fig:annealingantt) that it is possible for social annealers to produce low validation errors but in doing so we abandon the "satisficing" assumption that underpins this agents.
This is because the parameters that minimize validation error have agents that are deeply unsatisfied ($k=2.64$ means that they will keep exploring until they make at least 264% more than the average fisher) and quite risk prone ($\delta=19$ implies a very wide exploration range). 
At least for this formulation of the satisficing principle, the only way to fit it to data is to twist enough that it generates hypercompetitive behaviour nonetheless.

```{r annealingantt, fig.cap= "Empirical densities of validation and calibration error for *RUM precise* agents. As expected the cheating agent sacrifices fit quality in the calibration phase in order to do better in validation."}

full_directory<-"./northquota/"



original<- read_csv(paste0(full_directory,"",sep="annealing.csv")) %>%
  summarise_with_error_final(north_quota = TRUE) %>% mutate(id="Original",type="Validation") %>%
  dplyr::select(id,error,type)
cheating <- read_csv(paste(full_directory,"../ants/cheating_annealer/annealing_cheating_outcome.csv",sep="")) %>%
  summarise_with_error_final(north_quota = TRUE) %>%mutate(id="Cheating",type="Validation") %>%
  dplyr::select(id,error,type) 


bind_rows(original,cheating) %>%
  ggplot(aes(x=error,fill=id),col="black") +
  geom_density(alpha=0.8) +
  facet_grid(type~.) +
  theme_bw(20) +
  xlab("Outcome error") +
  ylab("Density") 

```


# Bibliography {-}
